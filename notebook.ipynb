{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb9d50a-fc6d-4bf3-b3a9-0de062147633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4296b8d-ad31-4c09-a5dc-80ce8f713cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './gpt4all-lora-quantized-ggml.bin'\n",
    "\n",
    "class LLama:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path = MODEL_PATH,\n",
    "        n_threads  = 24\n",
    "    ):\n",
    "        self.model = Llama(\n",
    "            model_path = model_path,\n",
    "            n_threads  = n_threads\n",
    "        )\n",
    "        \n",
    "    def predict(self, prompt, temperature = 0.5, max_len=512):\n",
    "        output = self.model(\n",
    "            prompt      = prompt,\n",
    "            max_tokens  = max_len - len(prompt),\n",
    "            echo        = False,\n",
    "            temperature = temperature\n",
    "        )\n",
    "        return output['choices'][0]['text'].replace('\\n', '')\n",
    "\n",
    "    \n",
    "class LlamaEmbeddingGenerator:\n",
    "    def __init__(self, model_path = MODEL_PATH):\n",
    "       self.model = LlamaCppEmbeddings(model_path=model_path)\n",
    "    \n",
    "    def embed(self, promp):\n",
    "        return np.array(self.model.embed_query(promp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9355e9-ddb7-44a4-9ed0-963bd8b84012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./gpt4all-lora-quantized-ggml.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model = LLama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaddc5e1-ef47-489e-87b4-79704643ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 34s, sys: 1 s, total: 4min 35s\n",
      "Wall time: 11.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   247.91 ms\n",
      "llama_print_timings:      sample time =    28.64 ms /    53 runs   (    0.54 ms per run)\n",
      "llama_print_timings: prompt eval time =   247.88 ms /     7 tokens (   35.41 ms per token)\n",
      "llama_print_timings:        eval time = 11577.34 ms /    52 runs   (  222.64 ms per run)\n",
      "llama_print_timings:       total time = 11859.30 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Overfitting is when a machine learning model learns too much from the training data, resulting in poor performance on unseen data. It can happen when there are too many features or if the model is not properly tuned for the task at hand.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.predict('What is overfitting?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5f6c79-1528-4325-ac3b-f3123ac4c455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./gpt4all-lora-quantized-ggml.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "generator = LlamaEmbeddingGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e98e418-750d-4f94-80dc-917fca39f181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 s, sys: 26.7 ms, total: 2.68 s\n",
      "Wall time: 226 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   224.97 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   224.72 ms /     7 tokens (   32.10 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   225.07 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embedding = generator.embed('What is overfitting?')\n",
    "embedding.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
