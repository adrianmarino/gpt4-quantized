{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800a3c21-a774-4b35-8358-7d2b5788a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "class GTP4:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path = './gpt4all-lora-quantized-ggml.bin',\n",
    "        n_threads  = 24\n",
    "    ):\n",
    "        self.model = Llama(\n",
    "            model_path = model_path,\n",
    "            n_threads  = n_threads\n",
    "        )\n",
    "        \n",
    "    def predict(self, prompt, temperature = 0.8, max_len=512):\n",
    "        output = self.model(\n",
    "            prompt      = prompt,\n",
    "            max_tokens  = max_len - len(prompt),\n",
    "            echo        = False,\n",
    "            temperature = temperature\n",
    "        )\n",
    "        return output['choices'][0]['text'].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9355e9-ddb7-44a4-9ed0-963bd8b84012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/gpt4all-lora-quantized-ggml.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model = GTP4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaddc5e1-ef47-489e-87b4-79704643ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 34s, sys: 1.12 s, total: 8min 35s\n",
      "Wall time: 22.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   263.69 ms\n",
      "llama_print_timings:      sample time =    38.95 ms /    86 runs   (    0.45 ms per run)\n",
      "llama_print_timings: prompt eval time =   256.42 ms /     8 tokens (   32.05 ms per token)\n",
      "llama_print_timings:        eval time = 22198.75 ms /    85 runs   (  261.16 ms per run)\n",
      "llama_print_timings:       total time = 22502.47 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Overfitting refers to the phenomenon where a machine learning model or algorithm learns too well on the training data, resulting in poor performance on unseen data. This can happen when there are too many parameters in the model and it becomes difficult for the model to generalize beyond the specific features that were used during training. Overfitting is often addressed by using regularization techniques such as dropout or weight decay.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output = model.predict(\n",
    "    'What is overfitting?',\n",
    "    temperature = 0.5,\n",
    "    max_len     = 512\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d55ef6-0a83-4656-9368-6ee5b2ec54e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
