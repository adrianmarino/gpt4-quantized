{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63515170-201d-4c78-a1d5-ac7b8e810652",
   "metadata": {},
   "source": [
    "https://python.langchain.com/en/latest/getting_started/getting_started.html\n",
    "\n",
    "https://gist.github.com/segestic/4822f3765147418fc084a250598c1fe6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818a76a5-ab3d-4f16-8335-d8d573c7cdb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566a01a3-79cc-4df0-ace5-c6616a44ec15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB_PATH   = './src'\n",
    "MODEL_PATH = './models/gpt4all-lora-quantized-ggml.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb9d50a-fc6d-4bf3-b3a9-0de062147633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(LIB_PATH)\n",
    "\n",
    "import pytorch_common.util as pu\n",
    "import os\n",
    "\n",
    "from llm import LlamaCpp, LlamaEmbeddingGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9786a-e7e5-40c0-83f3-c6afdb50edac",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432b8f97-3420-42d2-9778-ef1aca924e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pu.LoggerBuilder().on_console().build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac76228-8a14-4bfc-bc92-b9b4def7db42",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ac0582-3a8d-4b71-8466-00b53f7610fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/gpt4all-lora-quantized-ggml.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama.cpp: loading model from ./models/gpt4all-lora-quantized-ggml.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model         = LlamaCpp(MODEL_PATH)\n",
    "emb_generator = LlamaEmbeddingGenerator(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ded223-e547-4995-9b76-ae06e01606e4",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed7585e-a332-448f-8dd6-a7d1b692c7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 14:22:52,049 - INFO - Promp(23): Q: What is overfitting?\n",
      "\n",
      "2023-04-22 14:22:59,683 - INFO - Output(276):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 268 ms, total: 1min\n",
      "Wall time: 7.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:        load time =   297.75 ms\n",
      "llama_print_timings:      sample time =    26.30 ms /    60 runs   (    0.44 ms per run)\n",
      "llama_print_timings: prompt eval time =   297.73 ms /     8 tokens (   37.22 ms per token)\n",
      "llama_print_timings:        eval time =  7304.55 ms /    60 runs   (  121.74 ms per run)\n",
      "llama_print_timings:       total time =  7633.68 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A: Overfitting refers to a situation where a machine learning model learns too much from the training data and fails to generalize well on unseen data. This can happen when the model has been trained with too many parameters or if it has been trained on too specific data set.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model(\"Q: What is overfitting?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0310ceb-af46-4e3b-9c68-3e5b64a12092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.15 s, sys: 239 Âµs, total: 5.15 s\n",
      "Wall time: 432 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   370.50 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   307.00 ms /     8 tokens (   38.37 ms per token)\n",
      "llama_print_timings:        eval time =   124.14 ms /     1 runs   (  124.14 ms per run)\n",
      "llama_print_timings:       total time =   431.40 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.51655638, -1.41649508,  0.0890527 , ...,  0.42278111,\n",
       "         1.9438982 ,  0.90877187]),\n",
       " (4096,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embedding = emb_generator.embed(\"Q: What is overfitting?\")\n",
    "\n",
    "embedding, embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626e3b3-1884-4ea1-a401-c83505b324f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
